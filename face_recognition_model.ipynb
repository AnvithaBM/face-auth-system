{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Model Training\n",
    "## Using UWA HSFD Hyperspectral Database\n",
    "\n",
    "This notebook demonstrates building a face recognition model using the UWA HSFD hyperspectral database. We preprocess hyperspectral data to grayscale for simplicity and train a CNN to output embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "The UWA HSFD database contains hyperspectral images. We'll convert them to grayscale for simplicity.\n",
    "\n",
    "**Note**: Download the UWA HSFD database from the official source and place it in a `data/` directory.\n",
    "Dataset structure expected:\n",
    "```\n",
    "data/\n",
    "  └── UWA_HSFD/\n",
    "      ├── subject_01/\n",
    "      │   ├── image_001.mat (or appropriate format)\n",
    "      │   └── ...\n",
    "      ├── subject_02/\n",
    "      └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hyperspectral_to_grayscale(hyperspectral_image):\n",
    "    \"\"\"\n",
    "    Convert hyperspectral image to grayscale.\n",
    "    \n",
    "    For hyperspectral images with shape (H, W, Bands),\n",
    "    we'll average across spectral bands to create grayscale.\n",
    "    \n",
    "    Args:\n",
    "        hyperspectral_image: numpy array of shape (H, W, Bands)\n",
    "        \n",
    "    Returns:\n",
    "        Grayscale image of shape (H, W)\n",
    "    \"\"\"\n",
    "    # Average across spectral bands\n",
    "    grayscale = np.mean(hyperspectral_image, axis=2)\n",
    "    \n",
    "    # Normalize to 0-255 range\n",
    "    grayscale = ((grayscale - grayscale.min()) / (grayscale.max() - grayscale.min()) * 255).astype(np.uint8)\n",
    "    \n",
    "    return grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_dir, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load and preprocess UWA HSFD data.\n",
    "    \n",
    "    This is a placeholder function. Actual implementation depends on\n",
    "    the specific format of the UWA HSFD database files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to data directory\n",
    "        target_size: Target image size\n",
    "        \n",
    "    Returns:\n",
    "        X: Image data array\n",
    "        y: Labels array\n",
    "        labels: Label mapping\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    current_label = 0\n",
    "    \n",
    "    # Note: This is a generic implementation\n",
    "    # Actual implementation would depend on UWA HSFD format\n",
    "    # Typically hyperspectral data is in .mat or .hdf5 format\n",
    "    \n",
    "    print(\"Note: This is a demonstration. Actual UWA HSFD loading would require:\")\n",
    "    print(\"1. scipy.io.loadmat for .mat files\")\n",
    "    print(\"2. h5py for .hdf5 files\")\n",
    "    print(\"3. Specific knowledge of the data structure\")\n",
    "    \n",
    "    # Example structure (would need modification for actual data)\n",
    "    if os.path.exists(data_dir):\n",
    "        for subject_dir in sorted(os.listdir(data_dir)):\n",
    "            subject_path = os.path.join(data_dir, subject_dir)\n",
    "            if os.path.isdir(subject_path):\n",
    "                label_map[subject_dir] = current_label\n",
    "                \n",
    "                for image_file in os.listdir(subject_path):\n",
    "                    # Load hyperspectral image (format-specific)\n",
    "                    # hyperspectral_img = load_hyperspectral(image_path)\n",
    "                    \n",
    "                    # Convert to grayscale\n",
    "                    # grayscale = convert_hyperspectral_to_grayscale(hyperspectral_img)\n",
    "                    \n",
    "                    # For demonstration, we'll work with standard RGB images\n",
    "                    # In practice, replace with actual hyperspectral loading\n",
    "                    pass\n",
    "                    \n",
    "                current_label += 1\n",
    "    \n",
    "    return np.array(images), np.array(labels), label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build CNN Model for Face Embeddings\n",
    "\n",
    "We'll create a CNN model that outputs face embeddings. We'll use transfer learning with MobileNetV2 as the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_face_embedding_model(input_shape=(224, 224, 3), embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Build a CNN model for face embedding extraction.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image shape\n",
    "        embedding_dim: Dimension of output embedding\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "    # Use MobileNetV2 as base\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers (for transfer learning)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(embedding_dim, activation=None)(x)  # No activation for embeddings\n",
    "    \n",
    "    # L2 normalization of embeddings\n",
    "    from tensorflow.keras.layers import Lambda\n",
    "    import tensorflow as tf\n",
    "    embeddings = Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=embeddings)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = build_face_embedding_model(input_shape=(224, 224, 3), embedding_dim=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Triplet Loss Function\n",
    "\n",
    "For face recognition, we use triplet loss to train the network to produce similar embeddings for the same person and different embeddings for different people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Not used (required by Keras)\n",
    "        y_pred: Predictions containing [anchor, positive, negative] embeddings\n",
    "        alpha: Margin parameter\n",
    "        \n",
    "    Returns:\n",
    "        Loss value\n",
    "    \"\"\"\n",
    "    anchor = y_pred[:, 0:128]\n",
    "    positive = y_pred[:, 128:256]\n",
    "    negative = y_pred[:, 256:384]\n",
    "    \n",
    "    # Calculate distances\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    \n",
    "    # Triplet loss\n",
    "    loss = tf.maximum(pos_dist - neg_dist + alpha, 0.0)\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training (Demonstration)\n",
    "\n",
    "This section demonstrates the training process. In practice, you would:\n",
    "1. Load the actual UWA HSFD dataset\n",
    "2. Preprocess hyperspectral images to grayscale/RGB\n",
    "3. Create triplet batches (anchor, positive, negative)\n",
    "4. Train the model using triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training code (would need actual data)\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001), loss=triplet_loss)\n",
    "\n",
    "# Training would look like:\n",
    "# history = model.fit(\n",
    "#     triplet_generator,  # Generator yielding (anchor, positive, negative) triplets\n",
    "#     epochs=50,\n",
    "#     validation_data=val_triplet_generator\n",
    "# )\n",
    "\n",
    "print(\"Training demonstration complete.\")\n",
    "print(\"For actual training, you would need:\")\n",
    "print(\"1. UWA HSFD dataset properly loaded\")\n",
    "print(\"2. Triplet generation pipeline\")\n",
    "print(\"3. Sufficient training time and GPU resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Pre-trained Model\n",
    "\n",
    "For the face authentication system, we use a pre-trained MobileNetV2 model which is already integrated in the `face_utils.py` module. This provides a practical solution that works with RGB webcam inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding extractor used in the system\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.face_utils import EmbeddingExtractor\n",
    "\n",
    "extractor = EmbeddingExtractor()\n",
    "print(\"Embedding extractor ready!\")\n",
    "print(f\"Input shape: {extractor.input_shape}\")\n",
    "print(f\"Model architecture:\")\n",
    "extractor.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Embedding Extraction\n",
    "\n",
    "Let's test the embedding extraction with a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test image (random for demonstration)\n",
    "test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "\n",
    "# Extract embedding\n",
    "embedding = extractor.extract_embedding(test_image)\n",
    "normalized_embedding = extractor.normalize_embedding(embedding)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"Embedding L2 norm (before normalization): {np.linalg.norm(embedding):.4f}\")\n",
    "print(f\"Embedding L2 norm (after normalization): {np.linalg.norm(normalized_embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Similarity Comparison\n",
    "\n",
    "Demonstrate how embeddings are compared using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.auth_utils import cosine_similarity\n",
    "\n",
    "# Create two test embeddings\n",
    "embedding1 = np.random.randn(1280)\n",
    "embedding2 = np.random.randn(1280)\n",
    "embedding3 = embedding1 + np.random.randn(1280) * 0.1  # Similar to embedding1\n",
    "\n",
    "# Calculate similarities\n",
    "sim_different = cosine_similarity(embedding1, embedding2)\n",
    "sim_similar = cosine_similarity(embedding1, embedding3)\n",
    "sim_same = cosine_similarity(embedding1, embedding1)\n",
    "\n",
    "print(f\"Similarity (different faces): {sim_different:.4f}\")\n",
    "print(f\"Similarity (similar faces): {sim_similar:.4f}\")\n",
    "print(f\"Similarity (same face): {sim_same:.4f}\")\n",
    "print(f\"\\nTypical threshold for authentication: 0.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. How to preprocess hyperspectral data (UWA HSFD) to grayscale\n",
    "2. Building a CNN for face embedding extraction\n",
    "3. Triplet loss for training face recognition models\n",
    "4. Using pre-trained models for practical applications\n",
    "5. Embedding extraction and similarity comparison\n",
    "\n",
    "The actual face authentication system uses MobileNetV2 pre-trained on ImageNet, which provides excellent results for RGB webcam inputs without requiring extensive training on hyperspectral data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
