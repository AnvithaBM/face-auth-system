{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Face Recognition Model Training","","## Using UWA HSFD Hyperspectral Database","","This notebook demonstrates building a face recognition model using the UWA HSFD hyperspectral database. We preprocess hyperspectral data to grayscale for simplicity and train a CNN to output face embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch","import torch.nn as nn","import torch.optim as optim","from torchvision import models, transforms","import numpy as np","import matplotlib.pyplot as plt","import cv2","from sklearn.model_selection import train_test_split","import os"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Data Preprocessing","","The UWA HSFD database contains hyperspectral images. We'll convert them to grayscale for simplicity.","","**Note**: The UWA HSFD database is located at the specified path and contains PNG files.","Dataset structure expected:","```","UWA_HSFD/","  ├── subject_01/","  │   ├── image_001.png","  │   └── ...","  ├── subject_02/","  └── ...","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set the data path","data_path = r\"C:\\Users\\Anvitha\\Face based Person Authentication\\UWA HSFD V1.1 (1)\\UWA HSFD V1.1\\HyperSpec_Face_Session1\"","print(f\"Data path set to: {data_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_and_preprocess_data(data_dir, target_size=(224, 224)):\n","    \"\"\"\n","    Load and preprocess UWA HSFD data from PNG files.\n","    \n","    Args:\n","        data_dir: Path to data directory\n","        target_size: Target image size\n","        \n","    Returns:\n","        X: Image data array (RGB)\n","        y: Labels array\n","        labels: Label mapping\n","    \"\"\"\n","    images = []\n","    labels = []\n","    label_map = {}\n","    current_label = 0\n","    \n","    if os.path.exists(data_dir):\n","        for subject_dir in sorted(os.listdir(data_dir)):\n","            subject_path = os.path.join(data_dir, subject_dir)\n","            if os.path.isdir(subject_path):\n","                label_map[subject_dir] = current_label\n","                \n","                for image_file in os.listdir(subject_path):\n","                    if image_file.endswith('.png'):\n","                        image_path = os.path.join(subject_path, image_file)\n","                        \n","                        # Load PNG image\n","                        img = cv2.imread(image_path)\n","                        if img is not None:\n","                            # Convert grayscale to RGB if necessary\n","                            if len(img.shape) == 2:\n","                                img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","                            \n","                            # Resize to target size\n","                            img = cv2.resize(img, target_size)\n","                            \n","                            # Normalize to 0-1\n","                            img = img.astype(np.float32) / 255.0\n","                            \n","                            images.append(img)\n","                            labels.append(current_label)\n","                        \n","                current_label += 1\n","    \n","    return np.array(images), np.array(labels), label_map"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the data","X, y, label_map = load_and_preprocess_data(data_path)","print(f\"Loaded {len(X)} images from {len(label_map)} subjects\")","print(f\"Image shape: {X.shape[1:]}\")","print(f\"Label map: {label_map}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Build CNN Model for Face Embeddings","","We'll create a CNN model that outputs face embeddings. We'll use transfer learning with MobileNetV2 as the base."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_face_embedding_model(embedding_dim=1280):\n","    \"\"\"\n","    Build a CNN model for face embedding extraction using MobileNetV2.\n","    \n","    Args:\n","        embedding_dim: Dimension of output embedding (1280 for MobileNetV2)\n","        \n","    Returns:\n","        PyTorch Model\n","    \"\"\"\n","    # Load pre-trained MobileNetV2\n","    model = models.mobilenet_v2(pretrained=True)\n","    \n","    # Remove the classifier (last layer)\n","    model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n","    \n","    # Add a linear layer to get the desired embedding dimension\n","    model.embedding = nn.Linear(1280, embedding_dim)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the model","model = build_face_embedding_model(embedding_dim=1280)","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","model = model.to(device)","model.eval()  # Set to evaluation mode","print(f\"Model created and moved to {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Triplet Loss Function","","For face recognition, we use triplet loss to train the network to produce similar embeddings for the same person and different embeddings for different people."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def triplet_loss(anchor, positive, negative, margin=0.2):\n","    \"\"\"\n","    Triplet loss function.\n","    \n","    Args:\n","        anchor: Anchor embeddings\n","        positive: Positive embeddings\n","        negative: Negative embeddings\n","        margin: Margin parameter\n","        \n","    Returns:\n","        Loss value\n","    \"\"\"\n","    pos_dist = torch.sum((anchor - positive) ** 2, dim=1)\n","    neg_dist = torch.sum((anchor - negative) ** 2, dim=1)\n","    loss = torch.mean(torch.relu(pos_dist - neg_dist + margin))\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Training (Demonstration)","","This section demonstrates the training process. In practice, you would:","1. Load the actual UWA HSFD dataset","2. Preprocess hyperspectral images to grayscale/RGB","3. Create triplet batches (anchor, positive, negative)","4. Train the model using triplet loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split data for training/validation","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","","# For demonstration, compile and train (would need triplet data generator)","# optimizer = optim.Adam(model.parameters(), lr=0.001)","# model.train()","# # Training loop here...","","print(\"Data loaded and ready for training.\")","print(f\"Training samples: {len(X_train)}\")","print(f\"Validation samples: {len(X_val)}\")","print(\"For actual training, implement triplet data generator.\")"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Using Pre-trained Model","","For the face authentication system, we use a pre-trained MobileNetV2 model which is already integrated in the `face_utils.py` module. This provides a practical solution that works with RGB webcam inputs without requiring extensive training on hyperspectral data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the embedding extractor used in the system","import sys","sys.path.append('..')","from src.face_utils import EmbeddingExtractor","","extractor = EmbeddingExtractor()","print(\"Embedding extractor ready!\")","print(f\"Input shape: {extractor.input_shape}\")","print(f\"Model architecture:\")","print(extractor.model)"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Testing Embedding Extraction","","Let's test the embedding extraction with a sample image."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a test image (random for demonstration)","test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)","","","# Extract embedding","embedding = extractor.extract_embedding(test_image)","normalized_embedding = extractor.normalize_embedding(embedding)","","print(f\"Embedding shape: {embedding.shape}\")","print(f\"Embedding dimension: {len(embedding)}\")","print(f\"Embedding L2 norm (before normalization): {np.linalg.norm(embedding):.4f}\")","print(f\"Embedding L2 norm (after normalization): {np.linalg.norm(normalized_embedding):.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 8. Similarity Comparison","","Demonstrate how embeddings are compared using cosine similarity."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from src.auth_utils import cosine_similarity","","# Create two test embeddings","embedding1 = np.random.randn(1280)","embedding2 = np.random.randn(1280)","embedding3 = embedding1 + np.random.randn(1280) * 0.1  # Similar to embedding1","","# Calculate similarities","sim_different = cosine_similarity(embedding1, embedding2)","sim_similar = cosine_similarity(embedding1, embedding3)","sim_same = cosine_similarity(embedding1, embedding1)","","print(f\"Similarity (different faces): {sim_different:.4f}\")","print(f\"Similarity (similar faces): {sim_similar:.4f}\")","print(f\"Similarity (same face): {sim_same:.4f}\")","print(\"\\nTypical threshold for authentication: 0.6\")"]},{"cell_type":"markdown","metadata":{},"source":["## Summary","","This notebook demonstrates:","1. How to load and preprocess UWA HSFD PNG images (handling grayscale)","2. Building a CNN for face embedding extraction","3. Triplet loss for training face recognition models","4. Using pre-trained models for practical applications","5. Embedding extraction and similarity comparison","","The actual face authentication system uses MobileNetV2 pre-trained on ImageNet, which provides excellent results for RGB webcam inputs without requiring extensive training on hyperspectral data."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"nbformat":4,"nbformat_minor":4}}
